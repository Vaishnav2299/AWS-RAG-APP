{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d33318d",
   "metadata": {},
   "source": [
    "# Adjustment of Amazon SageMaker execution role policies\n",
    "\n",
    "In order to execute build Docker images and deploy infrastructure resources through AWS Serverless Application Model from SageMaker Studio , we need to adjust the SageMaker Execution Role. This is the IAM Role granting SageMaker Studio the permissions to perform actions against the AWS API. \n",
    "\n",
    "To adjust accordingly copy the following JSON:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"*\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::*:role/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringLikeIfExists\": {\n",
    "                    \"iam:PassedToService\": \"codebuild.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We then change the Execution Role's permissions by adding the above JSON as inline policy using the above\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0dc305c",
   "metadata": {},
   "source": [
    "# Adjustment of Amazon SageMaker execution role trust relationships\n",
    "\n",
    "To be able to build Docker images from SageMaker Studio we need to establish a trust relation ship between the service and the Amazon CodeBuild service (this is where the Docker build will be executed). Therefor we adjust the role's trust relationship accordingly by copying this JSON...\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"codebuild.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c62b6bb",
   "metadata": {},
   "source": [
    "# Installation and import of required dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.163.0 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418ebd0-8d97-4acd-8443-ddf40604ae78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fce3e-70ea-4533-8ba5-0f0f3cf7f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aws-sam-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8616f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a59d15f",
   "metadata": {},
   "source": [
    "# Setup of notebook environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c34e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve SM execution role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new STS client\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Call the GetCallerIdentity operation to retrieve the account ID\n",
    "response = sts_client.get_caller_identity()\n",
    "account_id = response['Account']\n",
    "account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve region\n",
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dbcfb82",
   "metadata": {},
   "source": [
    "# Setup of S3 bucket for storage of knowledge documents\n",
    "Amazon Kendra provides multiple built-in adapters for integrating with data sources to build up a document index, e.g. S3, web-scraper, RDS, Box, Dropbox, ...etc . In this lab we will store the documents containing the knowledge to be infused into the application in S3. For this purpose we create a dedicated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db749bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying bucket name for model artifact storage\n",
    "model_bucket_name = f'immersion-day-bucket-{account_id}-{region}'\n",
    "model_bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8af01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "location = {'LocationConstraint': region}\n",
    "\n",
    "bucket_name = model_bucket_name\n",
    "\n",
    "# Check if bucket already exists\n",
    "bucket_exists = True\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "except:\n",
    "    bucket_exists = False\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "if not bucket_exists:\n",
    "    if region == 'us-east-1':\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "        s3_client.create_bucket(Bucket=bucket_name,\n",
    "        CreateBucketConfiguration=location)\n",
    "    print(f\"Bucket '{bucket_name}' created successfully\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6bd38df",
   "metadata": {},
   "source": [
    "# Frontend\n",
    "\n",
    "All relevant components for building a dockerized frontend application can be found in the \"docker_and_app\" directory. It consists of the following files: \n",
    "- ```streamlit_app.py```: actual frontend utilizing the popular streamlit framework\n",
    "- ```Dockerfile```: Dockerfile providing the blueprint for the creation of a Docker image\n",
    "- ```requirements.txt```: specifying the dependencies required to be installed for hosting the frontend application\n",
    "- ```setup.sh```: setup script consisting all the necessary steps to create a ECR repository, build the Docker image and push it to the respective repository we created\n",
    "\n",
    "\n",
    "## UI\n",
    "\n",
    "The chatbot frontend web application \"AWSomeChat\" looks as follows:\n",
    "\n",
    "![chat-frontend](app_ss.png)\n",
    "\n",
    "To chat with the chatbot enter a message into the light grey input box and press ENTER. The chat conversation will appear below.\n",
    "\n",
    "On the top of the page you can spot the session id assigned to your chat conversation. This is used to map different conversation histories to a specific user since the chatbot backend is stateless. To start a new conversation, press the \"Clear Chat\" and \"Reset Session\" buttons on the top right of the page.\n",
    "\n",
    "\n",
    "## Dockerization and hosting\n",
    "\n",
    "In order to prepare our frontend application to be hosted as a Docker container, we execute the bash script setup.sh. It looks as follows: \n",
    "\n",
    "```bash \n",
    "#!/bin/bash\n",
    "\n",
    "# Get the AWS account ID\n",
    "aws_account_id=$(aws sts get-caller-identity --query Account --output text)\n",
    "aws_region=$(aws configure get region)\n",
    "\n",
    "echo \"AccountId = ${aws_account_id}\"\n",
    "echo \"Region = ${aws_region}\"\n",
    "\n",
    "\n",
    "# Create a new ECR repository\n",
    "echo \"Creating ECR Repository...\"\n",
    "aws ecr create-repository --repository-name rag-app\n",
    "\n",
    "# Get the login command for the new repository\n",
    "echo \"Logging into the repository...\"\n",
    "#$(aws ecr get-login --no-include-email)\n",
    "aws ecr get-login-password --region ${aws_region} | docker login --username AWS --password-stdin ${aws_account_id}.dkr.ecr.${aws_region}.amazonaws.com\n",
    "\n",
    "# Build and push the Docker image and tag it\n",
    "echo \"Building and pushing Docker image...\"\n",
    "sm-docker build -t \"${aws_account_id}.dkr.ecr.us-east-1.amazonaws.com/rag-app:latest\" --repository rag-app:latest .\n",
    "````\n",
    "\n",
    "The script performs the following steps in a sequential manner:\n",
    "\n",
    "1. Retrieval of the AWS account id and region\n",
    "2. Create a new ECR repository with the name rag-app. Note: this operation will fail, if the repository already exists within your account. This is intended behaviour and can be ignored.\n",
    "3. Login to the respective ECR repository. \n",
    "4. Build the Docker image and tag it with the \"latest\" tag using the sagemaker-studio-image-build package we previously installed. The \"sm-docker build\" command will push the built image into the specified repository automatically. All compute will be carried out in AWS CodeBuild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7402e76-b8f1-4833-ba36-2dfd40e1e8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd fe && bash setup.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c923145",
   "metadata": {},
   "source": [
    "The frontend Docker image is now residing in the respective ECR repository and can be used for our deployment at a later point in time during the lab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39ee162b-9247-47c2-8aed-207df3aa94b5",
   "metadata": {},
   "source": [
    "# Document store and retriever component\n",
    "\n",
    "## Kendra\n",
    "\n",
    "Amazon Kendra is an intelligent search service that uses natural language processing and advanced machine learning algorithms to return specific answers to search questions from your data.\n",
    "\n",
    "Unlike traditional keyword-based search, Amazon Kendra uses its semantic and contextual understanding capabilities to decide whether a document is relevant to a search query. It returns specific answers to questions, giving users an experience that's close to interacting with a human expert.\n",
    "\n",
    "\n",
    "## Creation of a Kendra index\n",
    "To use Kendra for retrieval-augmented generation we need to first create a Kendra index. This index will hold all the knowledge we want to infuse into our LLM-powered chatbot application. \n",
    "\n",
    "## Using Kendra with LLMs \n",
    "Now that we have a understanding of the basics of Kendra, we want to use it for the indexing of our documents. Kendra Indexing allows us to query our enterprise knowledge base, without us having to worry about how to handle different data types (pdf, xml) in S3, connectors to SaaS applications as well as webpages. \n",
    "\n",
    "\n",
    "\n",
    "We use an Amazon Kendra index to ingest enterprise unstructured data from data sources such as wiki pages, MS SharePoint sites, Atlassian Confluence, and document repositories such as Amazon S3. When a user interacts with the GenAI app, the flow is as follows:\n",
    "\n",
    "1. The user makes a request to the GenAI app.\n",
    "2. The app issues a search query to the Amazon Kendra index based on the user request.\n",
    "3. The index returns search results with excerpts of relevant documents from the ingested enterprise data.\n",
    "4. The app sends the user request and along with the data retrieved from the index as context in the LLM prompt.\n",
    "5. The LLM returns a succinct response to the user request based on the retrieved data.\n",
    "6. The response from the LLM is sent back to the user.\n",
    "\n",
    "\n",
    "\n",
    "## Uploading knowledge documents into an Amazon Kendra index\n",
    "\n",
    "Next we are going to add some more documents from S3 to show how easy it is to integrate different data sources to a Kendra Index. \n",
    "First we are going to download some interesting pdf files from the internet, but please feel free to drop any pdf you might find interesting in it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Create a bucket if it doesn't exist\n",
    "bucket_name = f'immersion-day-bucket-{account_id}-{region}'\n",
    "if s3.list_buckets()['Buckets']:\n",
    "    for bucket in s3.list_buckets()['Buckets']:\n",
    "        if bucket['Name'] == bucket_name:\n",
    "            break\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# List of URLs to download PDFs from\n",
    "pdf_urls = [\n",
    "    \"https://patentimages.storage.googleapis.com/bb/0f/5a/6ef847538a6ab5/US10606565.pdf\",\n",
    "    \"https://patentimages.storage.googleapis.com/f7/50/e4/81af7ddcbb2773/US9183397.pdf\",\n",
    "    \"https://docs.aws.amazon.com/pdfs/enclaves/latest/user/enclaves-user.pdf\",\n",
    "    \"https://docs.aws.amazon.com/pdfs/ec2-instance-connect/latest/APIReference/ec2-instance-connect-api.pdf\",\n",
    "]\n",
    "\n",
    "# Download PDFs from the URLs and upload them to the S3 bucket\n",
    "for url in tqdm(pdf_urls):\n",
    "    response = requests.get(url, stream=True)\n",
    "    filename = os.path.basename(url)\n",
    "    print(f\"Working on {filename}\")\n",
    "    fileobj = BytesIO()\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    for data in response.iter_content(block_size):\n",
    "        progress_bar.update(len(data))\n",
    "        fileobj.write(data)\n",
    "    progress_bar.close()\n",
    "    fileobj.seek(0)\n",
    "    s3.upload_fileobj(fileobj, bucket_name, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaae375d",
   "metadata": {},
   "source": [
    "Lets use those documents in Kendra. First navigate to the kendra console. \n",
    "\n",
    "Under \"Data Management\" you will find the tab \"Data Sources\". Navigate there and add a new data source via \"Add data source\". \n",
    "Take some time to inspect all the different connectors that are there for you to use out of the box. We will use s3 as our source. \n",
    "\n",
    "It is worth noting that Kendra respect enterprise level access attributes. That means, that it can deny queries if a user is not authorized to retrieve a document. \n",
    "\n",
    "You can either add the sample bucket as a data source that has been provided on the top of the connectors, but for the sake of demonstration, we will add our downloaded pdfs as well. \n",
    "\n",
    "\n",
    "\n",
    "After the connection has been established, you can sync your data source by clicking \"sync now\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "103f4de7",
   "metadata": {},
   "source": [
    "# Model- Falcon40b-instruct via SageMaker Real-time Endpoint with HuggingFace LLM Deep Learning Container\n",
    "\n",
    "\n",
    "For the response generation step we need a generative Model to come up with a appealing chatbot-like answer taking into account the users ask and the relevant documents retrieved from the Kendra index. For this purpose usually (instruction-fine-tuned) decoder or encoder-decoder models are used. I will be using the Falcon40b-instruct model.\n",
    "\n",
    "## Deploy Falcon40b-instruct on Amazon SageMaker\n",
    "\n",
    "The Falcon40b-instruct model can be deployed to SageMaker using the HuggingFace LLM DLC as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    "\n",
    "# TGI config\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b3ac885-d5aa-4c23-8b82-cd3acb79fbcb",
   "metadata": {},
   "source": [
    "# Orchestration layer\n",
    "\n",
    "We encapsulate the execution of the single steps involved in the retrieval-augmented generation design pattern into an orchestration layer.I will utilize the popular framework langchain for this.\n",
    "\n",
    "\n",
    "### KendraIndexRetriever\n",
    "\n",
    "Indexes refer to ways to structure documents so that LLMs can best interact with them. The most common way that indexes are used in chains is in a \"retrieval\" step. This step refers to taking a user's query and returning the most relevant documents. We draw this distinction because (1) an index can be used for other things besides retrieval, and (2) retrieval can use other logic besides an index to find relevant documents. We therefor have a concept of a \"Retriever\" interface - this is the interface that most chains work with.\n",
    "\n",
    "Most of the time when we talk about indexes and retrieval we are talking about indexing and retrieving unstructured data (like text documents). For interacting with structured data (SQL tables, etc) or APIs there is other design patterns we will not cover in this lab. The primary index and retrieval types supported by LangChain are currently centered around vector databases and document retrieval systems, Kendra being one of them.\n",
    "\n",
    "### ConversationalBufferWindowMemory \n",
    "Memory is the concept of storing and retrieving data in the process of a conversation. There are two main methods:\n",
    "\n",
    "- Based on input, fetch any relevant pieces of data\n",
    "- Based on the input and output, update state accordingly\n",
    "\n",
    "There are two main types of memory: short term and long term.\n",
    "\n",
    "Short term memory generally refers to how to pass data in the context of a singular conversation (generally is previous ChatMessages or summaries of them), while long term memory deals with how to fetch and update information between conversations. In this lab we will focus on short term memory in the form of the ```ConversationalBufferWindowMemory```. \n",
    "\n",
    "```ConversationBufferWindowMemory``` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large. \n",
    "\n",
    "### DynamoDBChatMessageHistory\n",
    "\n",
    "In their plain form, langchain's memory classes store the conversation in-memory. However this approach is not working when hosting the orchestration layer as a stateless microservice. To overcome this langchain offers ChatMessageHistory classes as memory plugins, interfacing to durable storage backends. The ```DynamoDBChatMessageHistory``` class leverages Amazon DynamoDB to store the chat message history in a durable manner. It supports session management by leveraging a session_id parameter matching the partition key in the DynamoDB schema.\n",
    "\n",
    "### SageMakerEndpoint\n",
    "\n",
    "For encapsulating funcionality around LLM inference, langchain provides the LLM class specifically designed for interfacing with LLMs. There are lots of LLM providers on the market, as well as a huge variety of hosting. This class is designed to provide a standard interface for all of them.\n",
    "\n",
    "Amazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. Inheriting from the generic LLM class, the ```SageMakerEndpoint```class provides functionality specifically tied to LLMs hosted via SageMaker Endpoints through a standard interface. Thereby it uses the endpoint name as unique identifier for the targeted endpoint.\n",
    "\n",
    "### PromptTemplate\n",
    "\n",
    "The new way of programming models is through prompts. A \"prompt\" refers to the input to the model. This input is rarely hard coded, but rather is often constructed from multiple components. A PromptTemplate is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy, ```PromptTemplate``` being one of them.\n",
    "\n",
    "A PromptValue is what is eventually passed to the model. Most of the time, this value is not hardcoded but is rather dynamically created based on a combination of user input, other non-static information (often coming from multiple sources), and a fixed template string. We call the object responsible for creating the PromptValue a ```PromptTemplate```. This object exposes a method for taking in input variables and returning a PromptValue.\n",
    "\n",
    "### ConversationalRetrievalChain\n",
    "\n",
    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
    "\n",
    "The ```ConversationalRetrievalChain``` is a special purpose chain designed for chatbot implementations infusing knowledge via retrieval-augmented generation. \n",
    "This chain has two steps. First, it condenses the current question and the chat history into a standalone question. This is neccessary to create a standanlone ask to use for retrieval. After that, it does retrieval and then answers the question using retrieval-augmented generation with a separate model. Part of the power of the declarative nature of LangChain is that you can easily use a separate language model for each call. This can be useful to use a cheaper and faster model for the simpler task of condensing the question, and then a more expensive model for answering the question. However, within this lab we will be using one model for both steps.\n",
    "\n",
    "## Create Lambda function codebase \n",
    "\n",
    "We will now look into the orchestrator implementation, meant to be hosted through AWS Lambda with a Python runtime. You can find the source code in the ```rag_app```directory. It consists of the following components:\n",
    "- ```kendra```directory: implementation of the Kendra retriever. This can be used as is and does not require further attention.\n",
    "- ```rag_app.py```: implementation of the orchestration layer as AWS Lambda handler function.\n",
    "- ```requirements.txt```: specifying the dependencies required to be installed for hosting the frontend application.\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "237baf5e",
   "metadata": {},
   "source": [
    "# Application Deployment\n",
    "\n",
    "Finally, we want to put all pieces together and deploy the LLM-powered chatbot application. \n",
    "\n",
    "## Infrastructure as Code: CloudFormation and SAM\n",
    "\n",
    "The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.\n",
    "\n",
    "## Application stack resources\n",
    "\n",
    "The application stack is defined through the ```template.yml``` AWS SAM file in yaml format. Once executed, it spins up the following resources:\n",
    "- AWS Lambda function for hosting the orchestration layer\n",
    "- Amazon API Gateway for exposing the orchestration layer in a RESTful way\n",
    "- ExecutionRole for the AWS Lambda function\n",
    "- VPC including two Subnets, an InternetGateway, ElasticIp, RoutingTables for hosting the application\n",
    "- ECS Service/Cluster including a TaskDefinition and SecurityGroups for hosting the Frontend\n",
    "- ExecutionRole for the ECS Task\n",
    "- LogGroup for Observability\n",
    "- LoadBalancer for exposing the Frontend\n",
    "- Amazon DynamoDB table for durable storage of the chat history\n",
    "\n",
    "## Deploy stack with SAM\n",
    "\n",
    "Before we will deploy the AWS SAM stack, we need to adjust the Lambda function's environment variable pointing to the Kendra index. \n",
    "\n",
    "Now we are ready for deployment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ca761-8215-4149-b127-8a54790b9e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Building the code artifacts\n",
    "!sam build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82546710-d73d-4a22-a453-e317deaa9dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploying the stack\n",
    "!sam deploy --stack-name rag-stack --resolve-s3 --capabilities CAPABILITY_IAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff363f27",
   "metadata": {},
   "source": [
    "Once the deployment is done, we can go ahead to the CloudFormation service and select the \"Resources\" tab of the Stack \"rag-app\". Click on the \"Physical ID\" of the LoadBalancer and copy the DNS name of the page you get forwarded to. You can now reach the web application through a browser by using this as URL.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd3e52ab",
   "metadata": {},
   "source": [
    "# Application testing\n",
    "\n",
    "Now that we are in the chat, let us check some things we want to ask our chatbot. \n",
    "\n",
    "Lets ask about Amazon EC2. What it is, how we can create one and some more information about it. \n",
    "Take a look at the below conversation .\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"ChatEC2.png\" alt=\"A chat with the model about EC2\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22572353",
   "metadata": {},
   "source": [
    "First of all, we can see that the LLM has memory about the previous conversation turn, as we reference EC2 implicitly via \"Okay. How can I create one?\" \n",
    "\n",
    "Secondly, we see that the shortcoming of a low number of retrieved characters on the Kendra side as I am using a free demo account\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a41271b",
   "metadata": {},
   "source": [
    "#### Discussions about the patents that we uploaded to Kendra bucket\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"PatentChat.png\" alt=\"A chat with the model about one of the patents we downloaded\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26504387",
   "metadata": {},
   "source": [
    "# Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3faab9-7a2f-41ed-a848-9ee5bb17de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sam delete --no-prompts --stack-name rag-stack --region $region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42276e25",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "AI21Blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
